

# **스핀 역학에서 딥러닝으로**

> ### 🚀 [Interactive summary](https://gemini.google.com/share/daa6909b3c20)  

## **1. 서론: 복잡계 물리학과 기계 지능의 대수적 조우**

20세기 과학사에서 가장 흥미로운 지적 융합 중 하나는 응집 물질 물리학(Condensed Matter Physics)의 통계적 방법론이 인공지능(Artificial Intelligence)의 계산 이론으로 전이된 과정일 것이다. 얼핏 보기에 자성체 내부에서 발생하는 미시적 입자들의 상호작용과 인간의 뇌에서 일어나는 인지 과정은 전혀 다른 차원의 현상처럼 보인다. 그러나 물리학자들은 수많은 입자가 상호작용하여 거시적인 물리적 상태를 결정하는 열역학적 원리가, 수많은 뉴런이 상호작용하여 기억과 학습이라는 거시적 지능을 창발하는 신경과학적 원리와 수학적으로 동형(Isomorphic)임을 발견하였다. 이 발견의 중심에는 1920년대에 고안된 **이징 모델(Ising Model)** 이 자리 잡고 있다.

본 보고서는 가장 단순한 자성체 모델인 이징 모델의 **스핀-스핀 상호작용(Spin-Spin Interaction)** 이 어떻게 **해밀토니안(Hamiltonian)** 에너지 함수를 통해 기술되는지 분석하고, 이것이 **홉필드 네트워크(Hopfield Network)** 의 연상 기억 모델로 재해석되는 과정을 추적한다. 나아가, 결정론적 모델에 열역학적 확률론을 도입한 **볼츠만 머신(Boltzmann Machine)** 과 그 구조적 제약 형태인 **제한된 볼츠만 머신(Restricted Boltzmann Machine, RBM)** 의 수리적 특성을 규명한다. 마지막으로, 제프리 힌튼(Geoffrey Hinton) 교수가 2006년 제안하여 딥러닝의 제3차 부흥을 이끈 **심층 신뢰 신경망(Deep Belief Network, DBN)** 의 **탐욕적 계층별 사전 학습(Greedy Layer-wise Pre-training)** 알고리즘이 어떻게 물리학의 에너지 최소화 원리를 변분 추론(Variational Inference)의 형태로 승화시켰는지, 그 이론적 계보를 상세히 논증하고자 한다. 이 일련의 과정은 단순한 알고리즘의 발전사가 아니라, 자연계의 에너지 최적화 법칙이 어떻게 기계 지능의 학습 원리로 환원될 수 있는지를 보여주는 거대한 지성사적 흐름이다.1

## **2. 이징 모델(Ising Model): 상호작용의 미시적 기원과 해밀토니안**

### **2.1 스핀 격자와 해밀토니안 역학**

이징 모델은 강자성체(Ferromagnetism)의 성질을 통계역학적으로 이해하기 위해 빌헬름 렌츠(Wilhelm Lenz)가 제안하고 에른스트 이징(Ernst Ising)이 구체화한 모델이다. 이 모델의 핵심은 물질을 구성하는 원자들의 자기 모멘트(Magnetic Dipole Moments), 즉 '스핀(Spin)'을 $+1$(업 스핀) 또는 $-1$(다운 스핀)의 이산적인 상태를 갖는 변수로 단순화하여 기술하는 데 있다.1

$N$개의 스핀이 격자(Lattice) 구조에 배치되어 있을 때, 시스템의 총 에너지인 해밀토니안 $H(\sigma)$는 개별 스핀들의 상태 배치 $\sigma = {\sigma_1, \sigma_2,..., \sigma_N}$에 따라 다음과 같이 정의된다:

$$H(\sigma) = - \sum_{\langle i, j \rangle} J_{ij} \sigma_i \sigma_j - \mu \sum_{i} h_i \sigma_i$$  
여기서 $\langle i, j \rangle$는 인접한 격자점 쌍(Nearest Neighbors)을 의미하며, $J_{ij}$는 두 스핀 간의 상호작용 세기(Coupling Constant)를, $h_i$는 외부 자기장(External Magnetic Field)을 나타낸다.4

이 수식에 내재된 물리적 함의는 **에너지 최소화(Energy Minimization)** 의 원리이다. 자연계의 시스템은 열평형 상태에서 자유 에너지를 낮추려는 경향이 있다. 상호작용 계수 $J_{ij}$가 양수($J_{ij} > 0$)인 경우, 두 스핀 $\sigma_i$와 $\sigma_j$가 같은 부호를 가질 때($\sigma_i \sigma_j = +1$) 해밀토니안의 첫 번째 항이 음수가 되어 전체 에너지가 낮아진다. 이를 강자성(Ferromagnetic) 상호작용이라 하며, 시스템은 질서 정연하게 정렬된 상태를 선호하게 된다. 반대로 $J_{ij} < 0$인 경우 서로 반대 방향을 선호하는 반강자성(Antiferromagnetic) 상호작용이 발생한다.4

### **2.2 상전이와 임계 현상: 거시적 질서의 출현**

이징 모델의 가장 중요한 물리적 통찰은 미시적인 스핀 간의 국소적 상호작용이 거시적인 **상전이(Phase Transition)** 를 유발한다는 점이다. 외부 온도가 임계 온도($T_c$)보다 높을 때는 열적 요동(Thermal Fluctuation)이 스핀의 정렬을 방해하여 무질서한 상자성(Paramagnetic) 상태가 되지만, $T < T_c$에서는 스핀 간의 상호작용 에너지가 열 에너지를 압도하여 자발적 자화(Spontaneous Magnetization)가 발생한다. 이는 2차원 이상의 격자에서 명확히 관찰되며, 시스템 전체가 하나의 거대한 질서를 형성하게 된다.4

이러한 '국소적 상호작용을 통한 전역적 질서의 창발'은 인공지능 연구자들에게 강력한 영감을 주었다. 뇌의 신경망 역시 개별 뉴런은 단순한 전기화학적 신호만을 주고받지만, 그 총합은 기억과 의식이라는 고도의 질서를 만들어내기 때문이다. 물리학의 $J_{ij}$는 신경망의 시냅스 가중치(Synaptic Weight)로, 스핀 상태 $\sigma_i$는 뉴런의 발화 상태(Activation State)로 자연스럽게 매핑될 수 있었다.2

---

## **3. 홉필드 네트워크(Hopfield Network): 기억의 에너지 지형도**

1982년, 존 홉필드(John Hopfield)는 이징 모델의 수리적 구조를 뇌의 연상 기억(Associative Memory) 모델에 그대로 이식한 홉필드 네트워크를 제안하였다. 이는 물리학의 스핀 시스템이 정보 저장 장치로 재해석된 역사적인 순간이었다.

### **3.1 신경망과 스핀 시스템의 구조적 동형성**

홉필드 네트워크에서 각 뉴런 $i$의 상태 $s_i$는 이징 모델의 스핀과 마찬가지로 $+1$(발화) 또는 $-1$(휴지)의 값을 가진다. 뉴런 간의 연결 강도 $w_{ij}$는 대칭적($w_{ij} = w_{ji}$)이며, 자기 자신으로의 연결은 없다($w_{ii} = 0$). 홉필드는 이 네트워크의 에너지 함수 $E$를 이징 모델의 해밀토니안과 동일한 형태로 정의하였다2:

$$E = - \frac{1}{2} \sum_{i \neq j} w_{ij} s_i s_j + \sum_{i} \theta_i s_i$$  
여기서 $\theta_i$는 뉴런의 발화 임계값(Threshold)으로, 이징 모델의 외부 자기장 $h_i$에 해당한다. 물리학에서는 $J_{ij}$가 물질 고유의 상수로 주어지지만, 신경망에서는 학습해야 할 변수가 된다. 홉필드 네트워크의 학습 목표는 저장하고자 하는 기억 패턴(Memory Pattern)들이 에너지 지형(Energy Landscape) 상의 **국소 최저점(Local Minima)**, 즉 끌개(Attractor)가 되도록 가중치 $w_{ij}$를 조정하는 것이다.

### **3.2 헵의 규칙(Hebbian Learning)과 패턴 저장**

기억을 저장하기 위해 홉필드는 도널드 헵(Donald Hebb)의 "함께 발화하는 뉴런은 서로 연결된다(Cells that fire together, wire together)"는 원리를 수식화하였다. $P$개의 패턴 ${\xi^{\mu}}_{\mu=1}^{P}$을 저장하기 위한 가중치는 다음과 같이 설정된다:

$$w_{ij} = \frac{1}{N} \sum_{\mu=1}^{P} \xi_i^{\mu} \xi_j^{\mu}$$  
이 규칙에 따르면, 두 뉴런이 같은 상태($+1/+1$ 또는 $-1/-1$)일 때 가중치가 증가하여 강한 양의 상호작용(강자성)을 갖게 되고, 서로 다른 상태일 때는 음의 가중치(반강자성)를 갖게 된다. 이는 해당 패턴 상태에서 에너지가 낮아지도록(안정화되도록) 해밀토니안을 조작하는 것과 같다.2

### **3.3 결정론적 동역학(Deterministic Dynamics)과 한계**

네트워크가 손상된 입력(초기 상태)을 받으면, 각 뉴런은 비동기적으로 자신의 상태를 업데이트하여 에너지를 낮추는 방향으로 변화한다:

$$s_i \leftarrow \text{sgn}\left( \sum_{j} w_{ij} s_j - \theta_i \right)$$  
이 갱신 규칙은 리아푸노프 함수(Lyapunov Function)의 성질에 따라 시스템의 에너지를 단조 감소시킨다. 결국 시스템은 초기 상태와 가장 가까운 에너지 최저점(저장된 기억)으로 수렴하게 되며, 이것이 바로 연상 기억의 원리이다.2 그러나 홉필드 네트워크는 결정론적 모델이기에 전역 최적해(Global Minimum)가 아닌 엉뚱한 국소 최적해(Spurious Local Minima)에 갇힐 위험이 크다. 이는 물리학적으로 시스템이 급랭(Quenching)되어 스핀 글라스(Spin Glass)와 같은 준안정 상태(Metastable State)에 고착되는 현상과 유사하다. 이를 해결하기 위해서는 시스템에 '열(Heat)'을 가해 에너지 장벽을 넘을 수 있는 확률적 요소를 도입해야 했다.2

---

## **4. 볼츠만 머신(Boltzmann Machine): 열역학적 확률론의 도입**

제프리 힌튼과 테리 세이노프스키는 홉필드 네트워크의 한계를 극복하기 위해 1985년, 통계역학의 볼츠만 분포(Boltzmann Distribution)를 따르는 확률적 신경망인 **볼츠만 머신(Boltzmann Machine)** 을 제안하였다.

### **4.1 확률적 뉴런과 시뮬레이티드 어닐링(Simulated Annealing)**

볼츠만 머신에서 뉴런의 상태는 결정론적으로 정해지지 않는다. 대신, 뉴런 $i$가 상태 $s_i=1$을 가질 확률은 에너지 차이 $\Delta E_i$와 온도 $T$에 의존하는 시그모이드 함수로 주어진다:

$$P(s_i = 1) = \frac{1}{1 + \exp(-\Delta E_i / T)}$$  
여기서 $T$는 온도를 나타내는 파라미터이다. $T$가 높을 때는 뉴런이 에너지 장벽을 무시하고 무작위로 상태를 바꿀 수 있어 국소 최적해를 탈출할 수 있다. 반면 $T \to 0$으로 낮추면 결정론적인 홉필드 네트워크와 동일하게 동작한다. 높은 온도에서 시작하여 서서히 온도를 낮추는 **시뮬레이티드 어닐링(Simulated Annealing)** 기법을 통해, 시스템은 전역 최적해인 가장 깊은 에너지 우물을 찾아갈 확률을 극대화할 수 있다.11

### **4.2 볼츠만 학습 규칙: 깨어남과 잠듦의 이중주**

볼츠만 머신은 **가시 유닛(Visible Unit, $v$)** 과 **은닉 유닛(Hidden Unit, $h$)** 을 구분함으로써 데이터의 내재적 표현(Internal Representation)을 학습할 수 있게 되었다. 시스템 전체 상태 벡터 $\mathbf{x} = {v, h}$의 확률 분포는 볼츠만 분포를 따른다:

$$P(\mathbf{x}) = \frac{e^{-E(\mathbf{x})/T}}{Z}, \quad Z = \sum_{\mathbf{x}} e^{-E(\mathbf{x})/T}$$  
여기서 $Z$는 모든 가능한 상태의 합인 분배 함수(Partition Function)이다. 학습의 목표는 훈련 데이터의 로그 우도(Log-Likelihood)를 최대화하는 것이다. 이를 위해 가중치에 대한 로그 우도의 기울기를 계산하면, 놀랍도록 직관적인 두 항의 차이로 유도된다13:

$$ \frac{\partial \log P(v)}{\partial w_{ij}} \propto \langle s_i s_j \rangle_{\text{data}} - \langle s_i s_j \rangle_{\text{model}} $$

이 수식은 학습이 두 단계(Phase)로 이루어짐을 보여준다.

1. **양의 단계(Positive Phase, Wake Phase):** 가시 유닛에 외부 데이터(현실)를 고정(Clamp)하고 은닉 유닛들이 열평형에 도달하도록 한다. 이때 두 뉴런이 동시에 활성화되는 상관관계 $\langle s_i s_j \rangle_{\text{data}}$를 측정하여 가중치를 증가시킨다(Hebbian Learning). 이는 에너지를 낮추어 데이터 패턴을 안정화하는 과정이다.  
2. **음의 단계(Negative Phase, Sleep Phase):** 외부 입력 없이 네트워크가 자유롭게 열적 요동을 하며 평형 상태에 도달하도록 둔다(꿈을 꾸는 상태, Hallucination). 이때 발생하는 상관관계 $\langle s_i s_j \rangle_{\text{model}}$를 측정하여 가중치를 감소시킨다(Anti-Hebbian Learning). 이는 모델이 헛보(Spurious) 패턴을 생성하는 것을 억제하기 위해 해당 상태의 에너지를 높이는 과정이다.15

그러나 볼츠만 머신은 이론적 우아함에도 불구하고 치명적인 실용적 한계가 있었다. 분배 함수 $Z$를 계산하기 위해서는 기하급수적인 경우의 수를 모두 합해야 하므로 계산이 불가능(Intractable)하며, 매번 평형 상태에 도달하기 위해 긴 시간의 깁스 샘플링(Gibbs Sampling)이 필요했기 때문이다.15

---

## **5. 구조적 제약을 통한 혁신: 제한된 볼츠만 머신(RBM)과 대조 발산**

제프리 힌튼 교수는 볼츠만 머신의 계산 복잡도 문제를 해결하기 위해 연결 구조에 제약을 가한 **제한된 볼츠만 머신(RBM)** 을 도입하였다.

### **5.1 완전 이분 그래프(Complete Bipartite Graph)와 조건부 독립성**

RBM은 같은 층(Layer) 내부의 뉴런끼리는 연결을 끊고, 오직 가시 층과 은닉 층 사이의 연결만을 허용하는 이분 그래프 구조를 갖는다.18 이 구조적 제약은 계산 효율성을 극적으로 향상시키는 수학적 성질을 제공한다.

RBM의 에너지 함수는 다음과 같다:

$$E(v, h) = - \sum_{i} a_i v_i - \sum_{j} b_j h_j - \sum_{i,j} v_i h_j w_{ij}$$  
이 구조 하에서는 **가시 층의 상태가 주어졌을 때, 모든 은닉 유닛들은 서로 조건부 독립(Conditionally Independent)** 이 된다. 즉, 한 은닉 유닛의 상태 확률은 다른 은닉 유닛의 상태에 영향을 받지 않으며, 오직 가시 유닛들에 의해서만 결정된다.20

$$P(h_j=1 | \mathbf{v}) = \sigma \left( b_j + \sum_i v_i w_{ij} \right)$$

$$P(\mathbf{h} | \mathbf{v}) = \prod_{j} P(h_j | \mathbf{v})$$  
마찬가지로, 은닉 층이 주어졌을 때 가시 유닛들도 서로 독립적이다.

$$P(\mathbf{v} | \mathbf{h}) = \prod_{i} P(v_i | \mathbf{h})$$  
이러한 조건부 독립성 덕분에, 기존 볼츠만 머신처럼 뉴런을 하나씩 순차적으로 업데이트할 필요 없이, **모든 은닉 유닛을 한 번에(Block Gibbs Sampling) 병렬로 샘플링**할 수 있게 되었다. 이는 샘플링 속도를 획기적으로 높이는 결과를 가져왔다.22

### **5.2 대조 발산(Contrastive Divergence, CD) 알고리즘**

RBM 구조가 샘플링을 빠르게 만들었지만, 여전히 정확한 로그 우도 기울기를 계산하기 위해서는 모델 분포의 평형 상태(음의 단계)에 도달할 때까지 무한히 많은 샘플링 반복이 필요했다. 2002년, 힌튼은 이를 근사하는 획기적인 학습 알고리즘인 **대조 발산(Contrastive Divergence, CD-k)** 을 제안하였다.18

CD 알고리즘의 핵심 통찰은 "완벽한 평형 상태에 도달할 필요 없이, 단 $k$번(일반적으로 $k=1$)의 깁스 샘플링 단계만으로도 기울기의 방향을 충분히 정확하게 추정할 수 있다"는 것이다.

#### **CD-1 알고리즘의 단계:**

1. **초기화:** 입력 데이터 $\mathbf{v}^{(0)}$를 가시 층에 고정한다.  
2. **양의 단계:** $P(\mathbf{h}|\mathbf{v}^{(0)})$를 통해 은닉 유닛 $\mathbf{h}^{(0)}$을 샘플링한다. 그리고 양의 상관관계 $\langle v_i^{(0)} h_j^{(0)} \rangle$를 계산한다.  
3. **재구성(Reconstruction):** 샘플링된 $\mathbf{h}^{(0)}$를 바탕으로 $P(\mathbf{v}|\mathbf{h}^{(0)})$를 통해 가시 유닛의 가상 상태 $\mathbf{v}^{(1)}$을 생성(재구성)한다.  
4. **음의 단계(근사):** 재구성된 $\mathbf{v}^{(1)}$을 바탕으로 다시 $P(\mathbf{h}|\mathbf{v}^{(1)})$를 통해 $\mathbf{h}^{(1)}$을 샘플링한다. 그리고 음의 상관관계 $\langle v_i^{(1)} h_j^{(1)} \rangle$를 계산한다.  
5. 가중치 업데이트:  
   $$ \Delta w_{ij} = \epsilon ( \langle v_i^{(0)} h_j^{(0)} \rangle - \langle v_i^{(1)} h_j^{(1)} \rangle ) $$

CD 알고리즘은 수학적으로는 편향된(Biased) 추정치이지만, 실험적으로는 매우 빠르게 수렴하며 RBM이 데이터의 특징을 효과적으로 학습하게 만들었다. 이 알고리즘의 등장은 RBM을 실용적인 머신러닝 도구로 격상시켰다.16

---

## **6. 심층 신뢰 신경망(Deep Belief Network): 적층과 변분 추론의 결합**

RBM 하나만으로는 복잡한 데이터의 추상적인 특징을 모두 포착하기에 충분히 깊지(Deep) 않았다. 그러나 당시 다층 신경망은 역전파(Backpropagation) 알고리즘의 기울기 소실(Vanishing Gradient) 문제로 인해 깊게 쌓을수록 학습이 되지 않는 난관에 봉착해 있었다. 2006년, 힌튼 교수는 RBM을 층층이 쌓아 올리는(Stacking) 방식의 **심층 신뢰 신경망(Deep Belief Network, DBN)** 을 발표하며 딥러닝의 새로운 지평을 열었다.26

### **6.1 탐욕적 계층별 사전 학습(Greedy Layer-wise Pre-training)**

DBN 학습의 핵심 전략은 전체 네트워크를 한 번에 학습시키는 대신, **한 번에 한 층씩(Layer-wise)** 학습시키는 것이다.

1. 첫 번째 RBM을 입력 데이터 $\mathbf{x}$로 학습시킨다. 학습이 완료되면 이 RBM의 은닉층 활성화 값은 입력 데이터의 1차 특징(Feature) 표현이 된다.  
2. 이 1차 특징을 고정하고, 이를 두 번째 RBM의 입력 데이터(가시 층)로 사용하여 학습시킨다.  
3. 이 과정을 원하는 깊이만큼 반복한다.

이 과정은 **비지도 사전 학습(Unsupervised Pre-training)** 이라 불리며, 네트워크의 가중치를 무작위 상태에서 시작하는 것이 아니라, 데이터의 통계적 구조를 이미 학습한 '좋은 초기 위치(Good Basin of Attraction)'에 배치하는 역할을 한다. 최적화 관점에서 이는 파라미터 공간의 험난한 지형에서 모델을 전역 최적해 근처의 깊은 협곡(Ravine)으로 안전하게 안내하는 것과 같다.28

### **6.2 상호보완적 사전 확률(Complementary Priors)과 변분 하한(ELBO)**

힌튼은 단순히 RBM을 쌓는 행위가 수학적으로 정당함을 증명하기 위해 **변분 추론(Variational Inference)** 의 틀을 사용하였다. DBN에서 상위 층을 추가하는 것은 하위 층의 은닉 유닛들에 대한 **상호보완적 사전 확률(Complementary Prior)** 을 제공하는 것으로 해석된다.

단일 RBM은 데이터의 우도 $P(\mathbf{v})$를 최대화하려 한다. 여기에 층을 더 쌓으면, 수학적으로 변분 하한(Variational Lower Bound), 즉 증거 하한(Evidence Lower Bound, ELBO)이 증가함이 보장된다.

$$\log P(\mathbf{v}) \ge \mathcal{L}(\text{Variational Bound})$$

새로운 층을 추가하여 학습시킬 때마다 이 하한선이 높아지므로, 전체 생성 모델의 성능(Log-probability)이 개선된다는 것이다. 이는 베이지안 추론에서 발생하는 '설명 소거(Explaining Away)' 문제를 완화하고, 추론을 단순화시키는 이론적 근거가 되었다.30 여기서 변분 하한을 최대화하는 것은 통계 물리학의 **헬름홀츠 자유 에너지(Helmholtz Free Energy)** 를 최소화하는 것과 동등하다는 점이 중요하다. 즉, 딥러닝의 학습 과정은 본질적으로 물리 시스템이 자유 에너지를 최소화하여 평형 상태를 찾아가는 과정과 수리적으로 동일하다.32

### **6.3 미세 조정(Fine-tuning): 업-다운(Up-Down) 알고리즘과 역전파**

사전 학습이 완료된 DBN은 생성 모델(Generative Model)로 동작한다. 이를 분류(Classification)나 회귀(Regression) 작업에 사용하기 위해서는 마지막 단계에서 지도 학습(Supervised Learning)을 통한 **미세 조정(Fine-tuning)** 이 필요하다.

힌튼은 **업-다운(Up-Down) 알고리즘** 을 제안했는데, 이는 **웨이크-슬립(Wake-Sleep) 알고리즘** 의 대조적(Contrastive) 변형이다.

* **Up-Pass (Wake):** 데이터를 입력하여 상위 층으로 신호를 전파하며 인지(Recognition) 모델의 가중치를 조정한다.  
* **Down-Pass (Sleep):** 최상위 연상 기억(Associative Memory)에서 샘플링을 시작하여 하위 층으로 신호를 생성(Generation)해 내려오며 생성 모델의 가중치를 조정한다.

또한, 사전 학습으로 초기화된 가중치에 표준적인 역전파(Backpropagation) 알고리즘을 적용하면, 기존의 기울기 소실 문제없이 매우 깊은 신경망도 효과적으로 학습될 수 있음이 입증되었다. 2006년 힌튼의 논문은 이러한 방식을 통해 MNIST 손글씨 인식에서 당시 최고 수준의 성능을 달성하며, 딥러닝이 실현 가능함을 전 세계에 알린 신호탄이 되었다.34

### **표 1. 물리적 모델과 인공지능 모델의 진화적 비교 분석**

다음 표는 이징 모델에서 시작하여 심층 신뢰 신경망에 이르기까지, 각 모델이 물리학적 원리를 어떻게 계승하고 발전시켰는지 요약한 것이다.

| 비교 항목 | 이징 모델 (Ising Model) | 홉필드 네트워크 (Hopfield Network) | 볼츠만 머신 (Boltzmann Machine) | 제한된 볼츠만 머신 (RBM) | 심층 신뢰 신경망 (DBN) |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **기본 단위** | 스핀 ($\sigma_i = \pm 1$) | 뉴런 ($s_i = \pm 1$) | 확률적 뉴런 ($p(s_i=1)$) | 확률적 뉴런 (가시/은닉) | 적층된 RBM 계층 |
| **상호작용 구조** | 격자 인접 ($J_{ij}$) | 완전 연결 ($w_{ij}$) | 완전 연결 (가시-은닉 구분) | 이분 그래프 (층간 연결만 허용) | 방향성(Directed) + 무방향성(Undirected) 혼합 |
| **에너지/목적함수** | 해밀토니안 $H$ | 에너지 함수 $E$ | 로그 우도 $\log P(v)$ | 로그 우도 $\log P(v)$ | 변분 하한 (ELBO) |
| **동역학 원리** | 열역학적 평형 | 에너지 최소화 (결정론적) | 시뮬레이티드 어닐링 (확률적) | 블록 깁스 샘플링 (조건부 독립) | 계층별 탐욕 학습 + 미세 조정 |
| **학습 알고리즘** | 없음 (물리 현상 기술) | 헵의 규칙 (Hebbian) | 볼츠만 학습 (Wake-Sleep) | 대조 발산 (Contrastive Divergence) | Up-Down 알고리즘, 역전파 |
| **주요 의의** | 상전이 및 임계 현상 규명 | 연상 기억 및 최적화 모델링 | 확률적 생성 모델의 시초 | 계산 효율적 생성 모델, 특징 추출 | 딥러닝의 실현, 심층 구조 학습 |

---

## **7. 결론 및 고찰: 물리학적 영감의 지속적인 유산**

이징 모델의 스핀 역학에서 시작하여 홉필드 네트워크의 에너지 지형, 볼츠만 머신의 열역학적 확률론, 그리고 RBM과 DBN의 구조적 혁신에 이르는 여정은 단순한 기술적 진보 이상의 의미를 갖는다. 이는 **'에너지 최소화(Energy Minimization)'** 라는 자연계의 근본 원리가 **'손실 함수 최적화(Loss Function Optimization)'** 및 **'확률 분포 학습(Probability Distribution Learning)'** 이라는 인공지능의 핵심 원리로 번역되는 과정이었다.

1. **결정론에서 확률론으로의 전이:** 초기 홉필드 네트워크는 이징 모델의 바닥 상태(Ground State) 탐색을 모방하여 결정론적 최적화를 수행했으나, 국소 최적해 문제에 직면했다. 볼츠만 머신은 여기에 '온도'라는 물리적 개념을 도입하여 확률적 탐색을 가능케 했으며, 이는 오늘날 딥러닝의 확률적 경사 하강법(SGD)이나 베이지안 뉴럴 네트워크(BNN)의 철학적 기반이 되었다.  
2. **구조적 제약의 승리:** 모든 뉴런이 연결된 볼츠만 머신보다, 연결을 제한한 RBM이 더 강력했다는 사실은 '제약이 창의성을 낳는다'는 역설을 보여준다. RBM의 이분 그래프 구조는 조건부 독립성을 보장하여 병렬 연산을 가능케 했고, 이는 현대의 합성곱 신경망(CNN)이나 트랜스포머(Transformer)가 갖는 **귀납적 편향(Inductive Bias)** 의 중요성을 예고한 것이었다.  
3. **비지도 학습과 생성 모델의 부활:** 힌튼의 DBN은 레이블이 없는 데이터로부터 스스로 특징을 학습할 수 있는 비지도 학습의 힘을 증명했다. 비록 현재 DBN 자체는 주류 아키텍처에서 물러났지만, 그가 정립한 **에너지 기반 모델(Energy-Based Model, EBM)** 과 변분 추론의 원리는 최신 생성 AI의 핵심인 **확산 모델(Diffusion Model)** 이나 **변분 오토인코더(VAE)** 속에 깊이 계승되어 있다.3

결론적으로, 현대 인공지능의 눈부신 성취는 반도체 기술의 발전뿐만 아니라, 반세기 전 자석의 성질을 이해하려 했던 물리학자들의 통찰에 그 뿌리를 두고 있다. RBM과 DBN으로 이어지는 이 계보는 학제 간 융합이 어떻게 인류 지성의 새로운 지평을 열 수 있는지를 보여주는 가장 명징한 사례로 기록될 것이다.

#### **Works cited**

1. accessed November 20, 2025, [https://en.wikipedia.org/wiki/Ising_model\#:\~:text=The%20model%20consists%20of%20discrete,to%20interact%20with%20its%20neighbors.](https://en.wikipedia.org/wiki/Ising_model#:~:text=The%20model%20consists%20of%20discrete,to%20interact%20with%20its%20neighbors.)  
2. Chapter 0 Statistical Mechanics and Artificial Neural Networks: Principles, Models, and Applications - arXiv, accessed November 20, 2025, [https://arxiv.org/html/2405.10957v1](https://arxiv.org/html/2405.10957v1)  
3. Evolutionary Optimization of Physics-Informed Neural Networks: Evo-PINN Frontiers and Opportunities - ResearchGate, accessed November 20, 2025, [https://www.researchgate.net/publication/396248878_Evolutionary_Optimization_of_Physics-Informed_Neural_Networks_Evo-PINN_Frontiers_and_Opportunities](https://www.researchgate.net/publication/396248878_Evolutionary_Optimization_of_Physics-Informed_Neural_Networks_Evo-PINN_Frontiers_and_Opportunities)  
4. Ising model - Wikipedia, accessed November 20, 2025, [https://en.wikipedia.org/wiki/Ising_model](https://en.wikipedia.org/wiki/Ising_model)  
5. Random Quantum Ising Model with Three-Spin Couplings - PMC - PubMed Central, accessed November 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11353498/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11353498/)  
6. Critical behavior of the classical spin-1 Ising model for magnetic systems - AIP Publishing, accessed November 20, 2025, [https://pubs.aip.org/aip/adv/article/12/3/035326/2819860/Critical-behavior-of-the-classical-spin-1-Ising](https://pubs.aip.org/aip/adv/article/12/3/035326/2819860/Critical-behavior-of-the-classical-spin-1-Ising)  
7. Hopfield Networks — Computing in Physics (498CMP), accessed November 20, 2025, [https://courses.physics.illinois.edu/phys498cmp/sp2022/ML/Hopfield.html](https://courses.physics.illinois.edu/phys498cmp/sp2022/ML/Hopfield.html)  
8. Photonic Hopfield neural network for the Ising problem - Optica Publishing Group, accessed November 20, 2025, [https://opg.optica.org/abstract.cfm?uri=oe-31-13-21340](https://opg.optica.org/abstract.cfm?uri=oe-31-13-21340)  
9. Hopfield network - Wikipedia, accessed November 20, 2025, [https://en.wikipedia.org/wiki/Hopfield_network](https://en.wikipedia.org/wiki/Hopfield_network)  
10. Hopfield network - Scholarpedia, accessed November 20, 2025, [http://www.scholarpedia.org/article/Hopfield_network](http://www.scholarpedia.org/article/Hopfield_network)  
11. Boltzmann machines | Redwood, accessed November 20, 2025, [https://redwood.berkeley.edu/wp-content/uploads/2024/08/boltzmann-machine.pdf](https://redwood.berkeley.edu/wp-content/uploads/2024/08/boltzmann-machine.pdf)  
12. ANALYSIS OF THE SIMULATED ANNEALING METHOD IN CLASSIC BOLTZMANN MACHINES, accessed November 20, 2025, [https://journals.rta.lv/index.php/ETR/article/download/1857/1865/2759](https://journals.rta.lv/index.php/ETR/article/download/1857/1865/2759)  
13. Boltzmann machine - Wikipedia, accessed November 20, 2025, [https://en.wikipedia.org/wiki/Boltzmann_machine](https://en.wikipedia.org/wiki/Boltzmann_machine)  
14. A Learning Algorithm for Boltzmann Machines* - Department of Computer Science, accessed November 20, 2025, [https://www.cs.toronto.edu/\~hinton/absps/cogscibm.pdf](https://www.cs.toronto.edu/~hinton/absps/cogscibm.pdf)  
15. Justifying and Generalizing Contrastive Divergence, accessed November 20, 2025, [https://www.iro.umontreal.ca/\~lisa/pointeurs/cd_expansion_nc_final.pdf](https://www.iro.umontreal.ca/~lisa/pointeurs/cd_expansion_nc_final.pdf)  
16. A Derivation and Application of Restricted Boltzmann Machines (2024 Nobel Prize), accessed November 20, 2025, [https://towardsdatascience.com/a-derivation-and-application-of-restricted-boltzmann-machines-2024-nobel-prize-ead5fe66908c/](https://towardsdatascience.com/a-derivation-and-application-of-restricted-boltzmann-machines-2024-nobel-prize-ead5fe66908c/)  
17. Convergence of contrastive divergence algorithm in exponential family - Project Euclid, accessed November 20, 2025, [https://projecteuclid.org/journals/annals-of-statistics/volume-46/issue-6A/Convergence-of-contrastive-divergence-algorithm-in-exponential-family/10.1214/17-AOS1649.pdf](https://projecteuclid.org/journals/annals-of-statistics/volume-46/issue-6A/Convergence-of-contrastive-divergence-algorithm-in-exponential-family/10.1214/17-AOS1649.pdf)  
18. accessed November 20, 2025, [https://www.geeksforgeeks.org/deep-learning/contrastive-divergence-in-restricted-boltzmann-machines/\#:\~:text=RBMs%20consist%20of%20two%20layers,a%20fully%20connected%20bipartite%20graph.](https://www.geeksforgeeks.org/deep-learning/contrastive-divergence-in-restricted-boltzmann-machines/#:~:text=RBMs%20consist%20of%20two%20layers,a%20fully%20connected%20bipartite%20graph.)  
19. Contrastive Divergence in Restricted Boltzmann Machines - GeeksforGeeks, accessed November 20, 2025, [https://www.geeksforgeeks.org/deep-learning/contrastive-divergence-in-restricted-boltzmann-machines/](https://www.geeksforgeeks.org/deep-learning/contrastive-divergence-in-restricted-boltzmann-machines/)  
20. Lecture 3 : Representation of Undirected Graphical Model 1 ..., accessed November 20, 2025, [https://www.cs.cmu.edu/\~epxing/Class/10708-17/notes-17/10708-scribe-lecture3.pdf](https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture3.pdf)  
21. Restricted Boltzmann Machines — Computing in Physics (498CMP) - Clark Research Group, accessed November 20, 2025, [https://clark.physics.illinois.edu/html/ML/RBM.html](https://clark.physics.illinois.edu/html/ML/RBM.html)  
22. arXiv:1708.04622v1 [cond-mat.dis-nn] 15 Aug 2017, accessed November 20, 2025, [https://arxiv.org/pdf/1708.04622](https://arxiv.org/pdf/1708.04622)  
23. Average Contrastive Divergence for Training Restricted Boltzmann Machines - MDPI, accessed November 20, 2025, [https://www.mdpi.com/1099-4300/18/1/35](https://www.mdpi.com/1099-4300/18/1/35)  
24. Restricted Boltzmann machine - Wikipedia, accessed November 20, 2025, [https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine)  
25. Restricted Boltzmann Machine, a complete analysis. Part 3: Contrastive Divergence algorithm | by Nguyễn Văn Lĩnh | datatype | Medium, accessed November 20, 2025, [https://medium.com/datatype/restricted-boltzmann-machine-a-complete-analysis-part-3-contrastive-divergence-algorithm-3d06bbebb10c](https://medium.com/datatype/restricted-boltzmann-machine-a-complete-analysis-part-3-contrastive-divergence-algorithm-3d06bbebb10c)  
26. Discover the Power of Deep Belief Networks - Viso Suite, accessed November 20, 2025, [https://viso.ai/deep-learning/deep-belief-networks/](https://viso.ai/deep-learning/deep-belief-networks/)  
27. An Introductory Review of Deep Learning for Prediction Models With Big Data - PMC, accessed November 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC7861305/](https://pmc.ncbi.nlm.nih.gov/articles/PMC7861305/)  
28. Greedy Layer-Wise Training of Deep Networks, accessed November 20, 2025, [https://proceedings.neurips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf](https://proceedings.neurips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf)  
29. Why does Unsupervised Pre-training Help Deep Learning? - Google Research, accessed November 20, 2025, [https://research.google.com/pubs/archive/35536.pdf](https://research.google.com/pubs/archive/35536.pdf)  
30. A fast learning algorithm for deep belief nets - Department of Computer Science, accessed November 20, 2025, [http://www.cs.toronto.edu/\~osindero/PUBLICATIONS/HOT_NC05_fast_deep_nets.pdf](http://www.cs.toronto.edu/~osindero/PUBLICATIONS/HOT_NC05_fast_deep_nets.pdf)  
31. A Fast Learning Algorithm for Deep Belief Nets - Sci-Hub, accessed November 20, 2025, [https://2024.sci-hub.se/2202/fc340cc78542743cfde267a7b6960470/hinton2006.pdf](https://2024.sci-hub.se/2202/fc340cc78542743cfde267a7b6960470/hinton2006.pdf)  
32. Evidence lower bound - Wikipedia, accessed November 20, 2025, [https://en.wikipedia.org/wiki/Evidence_lower_bound](https://en.wikipedia.org/wiki/Evidence_lower_bound)  
33. Machine Learning — Variational Inference | by Jonathan Hui | Medium, accessed November 20, 2025, [https://jonathan-hui.medium.com/machine-learning-variational-inference-273d8e6480bb](https://jonathan-hui.medium.com/machine-learning-variational-inference-273d8e6480bb)  
34. A fast learning algorithm for deep belief nets - PubMed, accessed November 20, 2025, [https://pubmed.ncbi.nlm.nih.gov/16764513/](https://pubmed.ncbi.nlm.nih.gov/16764513/)  
35. Deep Belief Nets - Department of Computer Science, accessed November 20, 2025, [https://www.cs.toronto.edu/\~hinton/nipstutorial/nipstut3.pdf](https://www.cs.toronto.edu/~hinton/nipstutorial/nipstut3.pdf)  
36. On the Origin of Deep Learning - Uberty, accessed November 20, 2025, [https://uberty.org/wp-content/uploads/2017/05/deep-learning-history.pdf](https://uberty.org/wp-content/uploads/2017/05/deep-learning-history.pdf)  
37. Physics-based simulation using back-propagation on energy functions | by Vatsal Verma, accessed November 20, 2025, [https://vatsalcode.medium.com/physics-based-simulation-using-back-propagation-on-energy-functions-c887051107df](https://vatsalcode.medium.com/physics-based-simulation-using-back-propagation-on-energy-functions-c887051107df)  
38. A comprehensive review of physics-informed deep learning and its applications in geoenergy development - The Innovation, accessed November 20, 2025, [https://www.the-innovation.org/article/doi/10.59717/j.xinn-energy.2025.100087](https://www.the-innovation.org/article/doi/10.59717/j.xinn-energy.2025.100087)